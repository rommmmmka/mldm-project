{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-19T01:53:58.768997Z",
     "start_time": "2024-12-19T01:53:58.753205Z"
    }
   },
   "source": [
    "import mne\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "from isruc_sleep import load_dataset, load_annotations\n",
    "from lightsleepnet import LightSleepNet"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T00:20:03.601276Z",
     "start_time": "2024-12-19T00:20:03.585306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CHANNELS = [\"F3-A2\", \"C3-A2\", \"O1-A2\", \"F4-A1\", \"C4-A1\", \"O2-A1\"]\n",
    "SLEEP_STAGES = [\"W\", \"N1\", \"N2\", \"N3\", \"REM\"]"
   ],
   "id": "9e16fe21aea1f7e7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T00:20:09.381033Z",
     "start_time": "2024-12-19T00:20:04.254706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subjects_data = load_dataset()\n",
    "subjects_labels = load_annotations()"
   ],
   "id": "40cf8da905c03fb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subject 1\n",
      "Loading subject 2\n",
      "Loading subject 3\n",
      "Loading subject 4\n",
      "Loading subject 5\n",
      "Loading subject 6\n",
      "Loading subject 7\n",
      "Loading subject 8\n",
      "Loading subject 9\n",
      "Loading subject 10\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T00:20:45.397857Z",
     "start_time": "2024-12-19T00:20:45.382699Z"
    }
   },
   "cell_type": "code",
   "source": "SUBJECT_TEST = 1",
   "id": "190787bae97ff60",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T01:50:50.433761Z",
     "start_time": "2024-12-19T00:20:45.797431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# for subject_test in range(1, 11):\n",
    "for subject_test in [SUBJECT_TEST]:\n",
    "    print(f\"===TEST SUBJECT: {subject_test}===\")\n",
    "\n",
    "    subjects_data_train = subjects_data[:subject_test - 1] + subjects_data[subject_test:]\n",
    "    subjects_label_train = subjects_labels[:subject_test - 1] + subjects_labels[subject_test:]\n",
    "    subjects_data_train_tensor = torch.cat(subjects_data_train)\n",
    "    subjects_label_train_tensor = torch.cat(subjects_label_train)\n",
    "    train_dataset = torch.utils.data.TensorDataset(subjects_data_train_tensor, subjects_label_train_tensor)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    subject_data_test_tensor = subjects_data[subject_test - 1]\n",
    "    subject_label_test_tensor = subjects_labels[subject_test - 1]\n",
    "    test_dataset = torch.utils.data.TensorDataset(subject_data_test_tensor, subject_label_test_tensor)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = LightSleepNet().to(\"cuda\")\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for training_epoch in range(100):\n",
    "        print(f\"{training_epoch + 1:<5}\", end=\"\")\n",
    "\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch.requires_grad = True\n",
    "\n",
    "            outputs = model(X_batch).float()\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            weights = []\n",
    "\n",
    "            for epoch_idx in range(X_batch.size(0)):\n",
    "                epoch_data = X_batch[epoch_idx].unsqueeze(0)\n",
    "                epoch_loss = criterion(model(epoch_data), y_batch[epoch_idx].unsqueeze(0))\n",
    "\n",
    "                epoch_grads = torch.autograd.grad(epoch_loss, epoch_data, retain_graph=True)[0]\n",
    "                grad_norms = epoch_grads.norm(p=2, dim=1).squeeze(0)\n",
    "\n",
    "                delta = 0.1 * grad_norms.std().item()\n",
    "                density = ((grad_norms.unsqueeze(1) - grad_norms.unsqueeze(0)).abs() < delta).sum(dim=1).float()\n",
    "\n",
    "                epoch_weight = density.mean()\n",
    "                weights.append(epoch_weight.detach())\n",
    "\n",
    "            weights = torch.tensor(weights, device=\"cuda\")\n",
    "            weights /= weights.sum()\n",
    "            # print(weights)\n",
    "            weighted_loss = (weights * loss).sum()\n",
    "            train_loss += weighted_loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            weighted_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "        print(f\"Loss: {train_loss / len(train_loader):.3f}  Accuracy: {100 * correct / total:.3f}\")"
   ],
   "id": "86fdd41dce85cd3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===TEST SUBJECT: 1===\n",
      "1    Loss: 0.787  Accuracy: 68.141\n",
      "2    Loss: 0.604  Accuracy: 75.148\n",
      "3    Loss: 0.577  Accuracy: 77.114\n",
      "4    Loss: 0.545  Accuracy: 78.689\n",
      "5    Loss: 0.538  Accuracy: 77.895\n",
      "6    Loss: 0.518  Accuracy: 78.979\n",
      "7    Loss: 0.510  Accuracy: 79.622\n",
      "8    Loss: 0.508  Accuracy: 79.735\n",
      "9    Loss: 0.511  Accuracy: 79.849\n",
      "10   Loss: 0.506  Accuracy: 79.269\n",
      "11   Loss: 0.495  Accuracy: 80.113\n",
      "12   Loss: 0.490  Accuracy: 80.088\n",
      "13   Loss: 0.492  Accuracy: 80.378\n",
      "14   Loss: 0.490  Accuracy: 80.202\n",
      "15   Loss: 0.492  Accuracy: 80.000\n",
      "16   Loss: 0.482  Accuracy: 80.252\n",
      "17   Loss: 0.483  Accuracy: 80.643\n",
      "18   Loss: 0.470  Accuracy: 81.033\n",
      "19   Loss: 0.465  Accuracy: 81.285\n",
      "20   Loss: 0.476  Accuracy: 80.693\n",
      "21   Loss: 0.482  Accuracy: 80.870\n",
      "22   Loss: 0.462  Accuracy: 80.870\n",
      "23   Loss: 0.464  Accuracy: 81.109\n",
      "24   Loss: 0.456  Accuracy: 81.979\n",
      "25   Loss: 0.463  Accuracy: 81.311\n",
      "26   Loss: 0.459  Accuracy: 81.361\n",
      "27   Loss: 0.460  Accuracy: 81.752\n",
      "28   Loss: 0.447  Accuracy: 81.815\n",
      "29   Loss: 0.446  Accuracy: 82.231\n",
      "30   Loss: 0.445  Accuracy: 82.092\n",
      "31   Loss: 0.446  Accuracy: 82.294\n",
      "32   Loss: 0.439  Accuracy: 82.659\n",
      "33   Loss: 0.443  Accuracy: 82.155\n",
      "34   Loss: 0.435  Accuracy: 82.331\n",
      "35   Loss: 0.439  Accuracy: 82.306\n",
      "36   Loss: 0.438  Accuracy: 82.205\n",
      "37   Loss: 0.429  Accuracy: 82.684\n",
      "38   Loss: 0.430  Accuracy: 82.735\n",
      "39   Loss: 0.431  Accuracy: 82.911\n",
      "40   Loss: 0.421  Accuracy: 83.100\n",
      "41   Loss: 0.428  Accuracy: 83.302\n",
      "42   Loss: 0.417  Accuracy: 82.949\n",
      "43   Loss: 0.424  Accuracy: 82.836\n",
      "44   Loss: 0.416  Accuracy: 83.264\n",
      "45   Loss: 0.415  Accuracy: 83.327\n",
      "46   Loss: 0.417  Accuracy: 82.899\n",
      "47   Loss: 0.412  Accuracy: 83.214\n",
      "48   "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 37\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(X_batch\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)):\n\u001B[0;32m     36\u001B[0m     epoch_data \u001B[38;5;241m=\u001B[39m X_batch[epoch_idx]\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m---> 37\u001B[0m     epoch_loss \u001B[38;5;241m=\u001B[39m criterion(\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch_data\u001B[49m\u001B[43m)\u001B[49m, y_batch[epoch_idx]\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m))\n\u001B[0;32m     39\u001B[0m     epoch_grads \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mgrad(epoch_loss, epoch_data, retain_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     40\u001B[0m     grad_norms \u001B[38;5;241m=\u001B[39m epoch_grads\u001B[38;5;241m.\u001B[39mnorm(p\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\mldm-project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\mldm-project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\mldm-project\\lightsleepnet.py:105\u001B[0m, in \u001B[0;36mLightSleepNet.forward\u001B[1;34m(self, x, adaptive_params)\u001B[0m\n\u001B[0;32m    102\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu(x)\n\u001B[0;32m    104\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresidual1(x, adaptive_params)\n\u001B[1;32m--> 105\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresidual2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madaptive_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    107\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(x)\n\u001B[0;32m    108\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooling(x)\n",
      "File \u001B[1;32m~\\PycharmProjects\\mldm-project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\mldm-project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\mldm-project\\lightsleepnet.py:72\u001B[0m, in \u001B[0;36mResidualBlock2.forward\u001B[1;34m(self, x, adaptive_params)\u001B[0m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, adaptive_params):\n\u001B[1;32m---> 72\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     73\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madabn1(out, adaptive_params)\n\u001B[0;32m     74\u001B[0m     out \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu(out)\n",
      "File \u001B[1;32m~\\PycharmProjects\\mldm-project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\mldm-project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\mldm-project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001B[0m, in \u001B[0;36mConv1d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    374\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 375\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\mldm-project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001B[0m, in \u001B[0;36mConv1d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    358\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    359\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv1d(\n\u001B[0;32m    360\u001B[0m         F\u001B[38;5;241m.\u001B[39mpad(\n\u001B[0;32m    361\u001B[0m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    368\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups,\n\u001B[0;32m    369\u001B[0m     )\n\u001B[1;32m--> 370\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    371\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\n\u001B[0;32m    372\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T01:50:55.245577Z",
     "start_time": "2024-12-19T01:50:54.145094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred = []\n",
    "test_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        y_pred.append(predicted.cpu())\n",
    "\n",
    "y_pred = torch.cat(y_pred).numpy()\n",
    "y_true = subject_label_test_tensor.to(\"cpu\").numpy()\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average=None)\n",
    "recall = recall_score(y_true, y_pred, average=None)\n",
    "f1 = f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "print(f\"Loss: {test_loss / len(test_loader):.3f}\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "for sleep_stage, precision_val, f1_val, recall_val in zip(SLEEP_STAGES, precision, recall, f1):\n",
    "    print(f\"{sleep_stage:7}precision={precision_val:.3f}  recall={recall_val:.3f}  f1={f1_val:.3f}\")"
   ],
   "id": "259b3b967d2e9748",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.290\n",
      "Accuracy: 0.632\n",
      "W      precision=0.876  recall=0.712  f1=0.600\n",
      "N1     precision=0.421  recall=0.488  f1=0.580\n",
      "N2     precision=0.614  recall=0.719  f1=0.869\n",
      "N3     precision=0.790  recall=0.587  f1=0.466\n",
      "REM    precision=0.636  recall=0.344  f1=0.235\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T01:54:11.639898Z",
     "start_time": "2024-12-19T01:54:11.623992Z"
    }
   },
   "cell_type": "code",
   "source": "confusion_matrix(y_true, y_pred)",
   "id": "181155bd6fef87c5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 99,  44,  19,   2,   1],\n",
       "       [  8,  69,  32,   0,  10],\n",
       "       [  5,  20, 324,  19,   5],\n",
       "       [  0,   0,  95,  83,   0],\n",
       "       [  1,  31,  58,   1,  28]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
